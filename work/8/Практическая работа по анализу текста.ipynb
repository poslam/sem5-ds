{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIeGIOzbUCdq"
   },
   "source": [
    "# Практическая работа по анализу текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqavdKUsYgxE"
   },
   "source": [
    "В качестве метрики качества используйте отчет о классификации https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITM4Cd-nUBxk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import razdel\n",
    "import math\n",
    "import random\n",
    "import pymorphy2\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7RBtc_0PM7J"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJWWOLwrOWM0"
   },
   "source": [
    "## 1. Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmD760M7QFQr"
   },
   "outputs": [],
   "source": [
    "morph = (\n",
    "    pymorphy2.MorphAnalyzer()\n",
    ")  # инициализируйется вне функции, иначе код будет работать дольше\n",
    "# spaCy если нет возможности обработки английского языка\n",
    "\n",
    "# Напишите функцию обработки предложения, используя следующие библиотеки pymorphy2, nltk, razdel. Обязательно ознакомьтесь с каждой из библиотек\n",
    "# после написания функции примените ее к датасету, используя df.progress_apply(...)\n",
    "# функция последовательно должна делать следующие шаги: разбиение текста на токены с помощью razdel, приведение их к нормальной форме с помощью morph и затем обработка стоп-слов с помощью nltk.\n",
    "# Вы можете внести изменения в описанный выше пайплайн, добавив новые шаги, но уменьшать перечисленные нельзя.\n",
    "# пайплайн выше приспособлен к работе с кириллицей. В датасете есть английские слова, их тоже надо суметь обработать\n",
    "# (одна из библиотек умеет автоматически обнаруживать иностранные слова и возвращать соответствующий тег)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEVeSkIeRIrN"
   },
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnC1PZ2ORLTB"
   },
   "outputs": [],
   "source": [
    "# Проанализируйте классы, их распределение и количество объектов в каждом. Оцените необходимость балансировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrXbOPaPR16X"
   },
   "outputs": [],
   "source": [
    "# Постройте облака точек https://www.datacamp.com/tutorial/wordcloud-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g-EMYk5SG7g"
   },
   "outputs": [],
   "source": [
    "# поищите выбросы в токенах, возможно, где-то вместо кириллицы использовалась латиница, тогда символы x и х будут разными токенами, а смысл может быть у них одинаковый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgpJhtFTReX6"
   },
   "outputs": [],
   "source": [
    "# разбейте выборку на тест и трейн train_test_split(X, y, random_state=SEED, stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wVhWyd0S6M2"
   },
   "outputs": [],
   "source": [
    "# поищите выбросы в предложениях, это можно сделать не на обработанных данных. Возможно, где-то перепутаны классы, или есть пустые предложения. Этот шаг обязательно после разбиения и только на трейне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75FlCPHnULXl"
   },
   "source": [
    "# 3. Тематическое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vdHcB6RUOyH"
   },
   "source": [
    "https://medium.com/@sayahfares19/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlCDgjnKUe2k"
   },
   "source": [
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw6sh_uZUfWk"
   },
   "outputs": [],
   "source": [
    "# используя ссылки выше создайте тематическую модель над предложениями. Помните, что темы != классы, поэтому их количество не обязательно должно совпадать.\n",
    "# не забывайте использовать фильтры над словами, это очень важно, чтобы не учиться на слишком частых токенах и не учитывать крайне редкие.\n",
    "# обязательно добавляйте n-граммы в модель.\n",
    "# для всего вышеперечисленного в gensim есть методы, параметры и функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pjTILy5Xqu-"
   },
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            chunksize=chunksize,\n",
    "            alpha=\"auto\",\n",
    "            eta=\"auto\",\n",
    "            iterations=iterations,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            eval_every=eval_every,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(\n",
    "            model=model, texts=texts, dictionary=dictionary, coherence=\"c_v\"\n",
    "        )\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWX2lGLJXxfu"
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "\n",
    "# Set training parameters.\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "limit=10; start=2; step=1;\n",
    "\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=train, start=, limit=, step=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGHVlHfpXzJx"
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy7PzmitYPzG"
   },
   "outputs": [],
   "source": [
    "# Выберите модель с наибольшей когерентностью.\n",
    "# Визуализируйте темы полученной модели с помощью pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMLSkY-7YfVz"
   },
   "outputs": [],
   "source": [
    "# Используйте выход модели (вероятности принадлежности к каждой из тем), как входные фичи, например, в SVM для предсказания классов предложений.\n",
    "# Токены и их векторные описания в таком случае не используются. Измерьте качество полученного алгоритма на тесте и трейне"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIqdoQPlZE04"
   },
   "source": [
    "## 4. Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5nnolstdKID"
   },
   "outputs": [],
   "source": [
    "import catboost\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFUDBWyyctcO"
   },
   "source": [
    "1. В методах преобразования данных пробуйте различные параметры, в поисках лучших для решения текущей задачи\n",
    "2. Не забывайте про подбор параметров у самого бустинга с помощью optuna\n",
    "\n",
    "https://forecastegy.com/posts/catboost-hyperparameter-tuning-guide-with-optuna/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQq_IsefbVRG"
   },
   "source": [
    "### Мешок слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCGj9L8WY2uy"
   },
   "outputs": [],
   "source": [
    "# обучите градиентный бустинг на мешке слов https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mK0p4OnbcWQ"
   },
   "source": [
    "### Мешок слов + n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxupD4dQbep1"
   },
   "outputs": [],
   "source": [
    "# обучите градиентный бустинг на мешке слов с n-граммами https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html с n-граммами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUHWGB0Qbcv0"
   },
   "source": [
    "### Мешок слов + m-skip-n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDi7CTwDbkWD"
   },
   "outputs": [],
   "source": [
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "            compose(tokenize, preprocess, self.decode)(doc), stop_words\n",
    "        )\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(\" \".join), pluck([0, 2]), sliding_window(3))(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf2RrhA9bc_h"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUT_wQbzbwlM"
   },
   "outputs": [],
   "source": [
    "# обучите градиентный бустинг на мешке слов https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nkX0ir9bxR-"
   },
   "source": [
    "### TF-IDF + n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNjczhqrbxfW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DsvDmkDbx3v"
   },
   "source": [
    "### TF-IDF + m-skip-n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXrUHVlhbyEg"
   },
   "outputs": [],
   "source": [
    "# по аналогии с кодом выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sa_FLN--UKhx"
   },
   "source": [
    "### Генерация искусственных данных и балансировка классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itbqzh7gUS2q"
   },
   "source": [
    "Выберите лучшее представление данных, опираясь на метрику. Попробуйте сбалансировать классы с помощью весов (параметр catboost), если выше этого не делали. Попробуйте сгенерировать новые данные для классов, в которых меньше всего объектов. Генерация представляет собой семплирование токенов из всего множества токенов определенного класса. Обучите модель на новом датасете, сравните качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL79J445rq9F"
   },
   "source": [
    "1. признаки предложений (длина предложений, наличие орф.знаков, количество предложений в тексте)\n",
    "2. обработка опечаток (расстояние между словом нормальным и словом с опечаткой)\n",
    "3. склеивание с частицей не."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYIuExQvspoq"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHWYxZiEsrP5"
   },
   "outputs": [],
   "source": [
    "# прогнать флоу выше с catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hv_sCKBUdF-"
   },
   "source": [
    "## Понижение размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcEsTtMZUy-m"
   },
   "outputs": [],
   "source": [
    "# уменьшите размерность векторов с помощью PCA, посмотрите, улучшается ли качество. Попробуйте несколько вариантов понижения размерности: от 90% до 50% от изначального размера вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bLkTRUauOBj"
   },
   "outputs": [],
   "source": [
    "# (ПО ЖЕЛАНИЮ) применить метод кластеризации и использовать метки кластеров как доп признак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L07K_QeldmMf"
   },
   "source": [
    "# 5. Интерпретация результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnqAUg_bdxJZ"
   },
   "source": [
    "https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Catboost%20tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzVYFgV1dqnL"
   },
   "outputs": [],
   "source": [
    "# используя туториал выше, интерпретируйте результаты. Определите, какие слова влияют в каждом классе больше остальных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0JBHJDGUs8t"
   },
   "source": [
    "## Визуализация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzUmS-IkUvgc"
   },
   "outputs": [],
   "source": [
    "# С помощью методов понижения размерности T-SNE И U-MAP взгляните на получившиеся векторные представления данных"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJbai/zinIlfO6Z++x5N8X",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
