{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIeGIOzbUCdq"
   },
   "source": [
    "# Практическая работа по анализу текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqavdKUsYgxE"
   },
   "source": [
    "В качестве метрики качества используйте отчет о классификации https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pymorphy3\n",
    "import razdel\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import shap.plots as sp\n",
    "from catboost import CatBoostClassifier\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from toolz import compose\n",
    "from toolz.curried import map as cmap, pluck, sliding_window\n",
    "from tqdm.notebook import tqdm\n",
    "from umap import UMAP\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITM4Cd-nUBxk"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "\n",
    "tqdm.pandas()\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "system_platform = platform.system()\n",
    "\n",
    "RESULT = {}  # name: accuracy_train, accuracy_test, model, params,\n",
    "#                    X_train, X_test, y_train, y_test, vectorizer, cl_rep_train, cl_rep_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G7RBtc_0PM7J"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "# nullable\n",
    "MAX_FEATURES = None\n",
    "\n",
    "# non-null, 0.0 <= MIN_DF, MAX_DF <= 1.0\n",
    "MIN_DF = 10\n",
    "MAX_DF = 0.5\n",
    "\n",
    "N_TRIALS = 5\n",
    "CV_FOLDS = 2\n",
    "\n",
    "# MAX_ITERS > MIN_ITERS\n",
    "MIN_ITERS = 100\n",
    "MAX_ITERS = 300\n",
    "\n",
    "THEME_COUNT_START = 9 # min_num_topics = THEME_COUNT_START\n",
    "THEME_COUNT_END = 12 # max_num_topics = THEME_COUNT_END\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score_from_class_report(map: dict, score_kind: str = \"f1-score\"):\n",
    "    scores = []\n",
    "\n",
    "    for key in map.keys():\n",
    "        try:\n",
    "            scores.append(map[key][score_kind])\n",
    "        except:\n",
    "            print((map[key]))\n",
    "            raise ValueError(f\"score {score_kind} for key {key} isn't found\")\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "\n",
    "def print_results():\n",
    "    for key in RESULT.keys():\n",
    "        print(f\"Model: {key}\")\n",
    "        print(f\"Train accuracy: {RESULT[key]['accuracy_train']}\")\n",
    "        print(f\"Test accuracy: {RESULT[key]['accuracy_test']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dt = datetime.now()\n",
    "\n",
    "df = pd.read_csv(\"./train.csv\", encoding=\"utf-8\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJWWOLwrOWM0"
   },
   "source": [
    "## 1. Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(df[\"text\"])\n",
    "\n",
    "words = re.findall(r\"\\b\\w+\\b\", all_text.lower())\n",
    "word_counts = Counter(words)\n",
    "\n",
    "word_counts_df = pd.DataFrame(\n",
    "    word_counts.items(), columns=[\"word\", \"count\"]\n",
    ").sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_alph_low = [chr(i) for i in range(ord(\"а\"), ord(\"я\") + 1)]\n",
    "en_alph_low = [chr(i) for i in range(ord(\"a\"), ord(\"z\") + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_tokens = {\n",
    "    \"км\": \"километр\",\n",
    "    \"г\": \"год\",\n",
    "    \"мск\": \"москва\",\n",
    "    \"д\": \"день\",\n",
    "    \"р\": \"руб\",\n",
    "    \"рубль\": \"руб\",\n",
    "    \"₽\": \"руб\",\n",
    "    \"америка\": \"сша\",\n",
    "    \"msk\": \"москва\",\n",
    "    \"дек\": \"декабрь\",\n",
    "    \"спб\": \"петербург\",\n",
    "    \"spb\": \"петербург\",\n",
    "    \"санктпетербург\": \"петербург\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"александр\",\n",
    "    \"андрей\",\n",
    "    \"марк\",\n",
    "    \"ян\",\n",
    "    \"камил\",\n",
    "    \"сергей\",\n",
    "    \"карлос\",\n",
    "    \"мария\",\n",
    "    \"дмитрий\",\n",
    "    \"уильямс\",\n",
    "    \"даниил\",\n",
    "    \"данил\",\n",
    "    \"джеймс\",\n",
    "    \"игорь\",\n",
    "    \"саша\",\n",
    "    \"денис\",\n",
    "    \"светлана\",\n",
    "    \"александра\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\n",
    "    \"us\",\n",
    "    \"usa\",\n",
    "    \"сша\",\n",
    "    \"россия\",\n",
    "    \"russia\",\n",
    "    \"rus\",\n",
    "    \"бразилия\",\n",
    "    \"турция\",\n",
    "    \"вьетнам\",\n",
    "    \"германия\",\n",
    "    \"польша\",\n",
    "    \"австралия\",\n",
    "    \"франция\",\n",
    "    \"ссср\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    \"казань\",\n",
    "    \"урал\",\n",
    "    \"москва\",\n",
    "    \"майами\",\n",
    "    \"бостон\",\n",
    "    \"санкт\",\n",
    "    \"петербург\",\n",
    "    \"екатеринбург\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\n",
    "    \"январь\",\n",
    "    \"февраль\",\n",
    "    \"март\",\n",
    "    \"апрель\",\n",
    "    \"май\",\n",
    "    \"июнь\",\n",
    "    \"июль\",\n",
    "    \"август\",\n",
    "    \"сентябрь\",\n",
    "    \"октябрь\",\n",
    "    \"ноябрь\",\n",
    "    \"декабрь\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [i for i in string.punctuation] + [\n",
    "    \"\",\n",
    "    \"⠀\",\n",
    "    \"―\",\n",
    "    \"⸺\",\n",
    "    \"⸻\",\n",
    "    \"—\",\n",
    "    \"–\",\n",
    "    \"‑\",\n",
    "    \"‐\",\n",
    "    \"−\",\n",
    "    \"-\",\n",
    "    \"–\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = [\n",
    "    \"время\",\n",
    "    \"год\",\n",
    "    \"месяц\",\n",
    "    \"неделя\",\n",
    "    \"день\",\n",
    "    \"час\",\n",
    "    \"минута\",\n",
    "    \"послезавтра\",\n",
    "    \"завтра\",\n",
    "    \"сегодня\",\n",
    "    \"вчера\",\n",
    "    \"позавчера\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless = [\n",
    "    \"это\",\n",
    "    \"такой\",\n",
    "    \"который\",\n",
    "    \"весь\",\n",
    "    \"ваш\",\n",
    "    \"наш\",\n",
    "    \"все\",\n",
    "    \"всё\",\n",
    "    \"еще\",\n",
    "    \"ещё\",\n",
    "    \"даже\",\n",
    "    \"пока\",\n",
    "    \"свой\",\n",
    "    \"этот\",\n",
    "    \"снова\",\n",
    "    \"хотя\",\n",
    "    \"либо\",\n",
    "    \"каждый\",\n",
    "    \"также\",\n",
    "    \"твой\",\n",
    "    \"поэтому\",\n",
    "    \"чтобы\",\n",
    "    \"ранее\",\n",
    "    \"нужно\",\n",
    "    \"далее\",\n",
    "    \"наиболее\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = [\n",
    "    \"мочь\",\n",
    "    \"смочь\",\n",
    "    \"быть\",\n",
    "    \"стать\",\n",
    "    \"сказать\",\n",
    "    \"смотреть\",\n",
    "    \"описать\",\n",
    "    \"думать\",\n",
    "    \"говорить\",\n",
    "    \"продать\",\n",
    "    \"указать\",\n",
    "    \"работать\",\n",
    "    \"рассказать\",\n",
    "    \"провести\",\n",
    "    \"получить\",\n",
    "    \"выиграть\",\n",
    "    \"пройти\",\n",
    "    \"начать\",\n",
    "    \"добавить\",\n",
    "    \"написать\",\n",
    "    \"считать\",\n",
    "    \"взять\",\n",
    "    \"иметь\",\n",
    "    \"писать\",\n",
    "    \"купить\",\n",
    "    \"являться\",\n",
    "    \"хотеть\",\n",
    "    \"играть\",\n",
    "    \"сделать\",\n",
    "    \"делать\",\n",
    "    \"сыграть\",\n",
    "    \"знать\",\n",
    "    \"выйти\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = [\n",
    "    \"новый\",\n",
    "    \"хороший\",\n",
    "    \"большой\",\n",
    "    \"следующий\",\n",
    "    \"самый\",\n",
    "    \"готовый\",\n",
    "    \"некоторый\",\n",
    "    \"любой\",\n",
    "    \"данный\",\n",
    "    \"дорогой\",\n",
    "    \"московский\",\n",
    "    \"первый\",\n",
    "    \"второй\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [\n",
    "    \"очко\",\n",
    "    \"матч\",\n",
    "    \"команда\",\n",
    "    \"турнир\",\n",
    "    \"чемпионат\",\n",
    "    \"чемпион\",\n",
    "    \"победитель\",\n",
    "    \"проигрыш\",\n",
    "    \"победа\",\n",
    "    \"поражение\",\n",
    "    \"ссылка\",\n",
    "    \"компания\",\n",
    "    \"информация\",\n",
    "    \"новинка\",\n",
    "    \"рука\",\n",
    "    \"тело\",\n",
    "    \"рекорд\",\n",
    "    \"встреча\",\n",
    "    \"мир\",\n",
    "    \"результат\",\n",
    "    \"игра\",\n",
    "    \"игрок\",\n",
    "    \"тренер\",\n",
    "    \"друг\",\n",
    "    \"человек\",\n",
    "    \"строчка\",\n",
    "    \"финал\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(word_counts_df):\n",
    "    outliers = []\n",
    "    for word in word_counts_df[\"word\"]:\n",
    "        if (\n",
    "            (\n",
    "                any(char in en_alph_low for char in word)\n",
    "                and any(char in ru_alph_low for char in word)\n",
    "            )\n",
    "            or any(char.isdigit() for char in word)\n",
    "            or word.isspace()\n",
    "        ):\n",
    "            outliers.append(word)\n",
    "\n",
    "    return outliers\n",
    "\n",
    "\n",
    "all_text = \" \".join(df[\"text\"])\n",
    "\n",
    "words = re.findall(r\"\\b\\w+\\b\", all_text.lower())\n",
    "word_counts = Counter(words)\n",
    "\n",
    "word_counts_df = pd.DataFrame(\n",
    "    word_counts.items(), columns=[\"word\", \"count\"]\n",
    ").sort_values(by=\"count\", ascending=True)\n",
    "\n",
    "outliers = detect_outliers(word_counts_df)\n",
    "\n",
    "print(len(outliers), outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = (\n",
    "    ru_alph_low\n",
    "    + en_alph_low\n",
    "    + punctuation\n",
    "    + months\n",
    "    + names\n",
    "    + countries\n",
    "    + cities\n",
    "    + dt\n",
    "    + useless\n",
    "    + verbs\n",
    "    + adj\n",
    "    + nouns\n",
    "    + outliers\n",
    "    + [\n",
    "        \"руб\",\n",
    "        \"офф\",\n",
    "        \"очень\",\n",
    "        \"id\",\n",
    "        \"ска\",\n",
    "        \"млн\",\n",
    "        \"го\",\n",
    "        \"ло\",\n",
    "        \"вк\",\n",
    "        \"яндекс\",\n",
    "        \"pro\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(\n",
    "    stopwords.words(\"russian\") + stopwords.words(\"english\") + custom_stop_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmD760M7QFQr"
   },
   "outputs": [],
   "source": [
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def filter_token(token: str):\n",
    "    to_replace_list = punctuation\n",
    "\n",
    "    for to_replace in to_replace_list:\n",
    "        token = token.replace(to_replace, \"\")\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def process_token(token: str):\n",
    "    new_token: str = (\n",
    "        token if token not in replace_tokens.keys() else replace_tokens[token]\n",
    "    ).replace(\"ё\", \"е\")\n",
    "\n",
    "    if new_token in stop_words or (\n",
    "        any(char in en_alph_low for char in token)\n",
    "        and any(char in ru_alph_low for char in token)\n",
    "    ):\n",
    "        return \"tokenoid\"\n",
    "\n",
    "    return new_token\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = [\n",
    "        filter_token(token.text)\n",
    "        for token in razdel.tokenize(text)\n",
    "        if token.text not in stop_words\n",
    "    ]\n",
    "\n",
    "    normalized_tokens = [\n",
    "        morph.parse(token)[0].normal_form\n",
    "        for token in tokens\n",
    "        if \"UNKN\" not in \"\".join([kind.tag._str for kind in morph.parse(token)])\n",
    "    ]\n",
    "\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in normalized_tokens:\n",
    "        filtered_token = process_token(token)\n",
    "\n",
    "        if \"tokenoid\" not in filtered_token:\n",
    "            filtered_tokens.append(str(filtered_token))\n",
    "\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "if system_platform == \"Windows\":\n",
    "    df[\"processed_text\"] = df[\"text\"].progress_apply(process_text)\n",
    "else:\n",
    "    df[\"processed_text\"] = df[\"text\"].parallel_apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(df[\"processed_text\"])\n",
    "\n",
    "words = re.findall(r\"\\b\\w+\\b\", all_text.lower())\n",
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEVeSkIeRIrN"
   },
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnC1PZ2ORLTB"
   },
   "outputs": [],
   "source": [
    "sentence_outliers = df[df[\"processed_text\"] == \"\"]\n",
    "print(f\"Sentence outliers detected: {sentence_outliers}\\n\")\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "class_distribution = df[\"category\"].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
    "plt.xlabel(\"category\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in df[\"category\"].unique():\n",
    "    text = \" \".join(df[df[\"category\"] == cls][\"processed_text\"])\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=200,\n",
    "        collocations=False,\n",
    "        background_color=\"white\",\n",
    "    ).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(str(cls))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"processed_text\"]\n",
    "y = df[\"category\"]\n",
    "\n",
    "X_train_src, X_test_src, y_train_src, y_test_src = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    random_state=SEED,\n",
    "    stratify=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "выводы:\n",
    "1. балансировать не обязательно, так как примерно все на одном уровне\n",
    "2. пустые предложения отсутствуют"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75FlCPHnULXl"
   },
   "source": [
    "## 3. Тематическое моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3pjTILy5Xqu-"
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(\n",
    "    dictionary,\n",
    "    corpus,\n",
    "    texts,\n",
    "    limit: int,\n",
    "    passes: int,\n",
    "    chunksize: int,\n",
    "    iterations: int,\n",
    "    start: int,\n",
    "    step: int,\n",
    "    eval_every=None,\n",
    "):\n",
    "    \"\"\"\n",
    "        Compute c_v coherence for various number of topics\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dictionary : Gensim dictionary\n",
    "        corpus : Gensim corpus\n",
    "        texts : List of input texts\n",
    "        limit : Max num of topics\n",
    "        Returns:\n",
    "        -------\n",
    "        model_list : List of LDA topic models\n",
    "        coherence_values : Coherence values corresponding to the LDA model\n",
    "    with respective number of topics\n",
    "    \"\"\"\n",
    "    temp = dictionary[0]\n",
    "    id2word = dictionary.id2token\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "\n",
    "    for num_topics in range(start, limit + 1, step):\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            chunksize=chunksize,\n",
    "            alpha=\"auto\",\n",
    "            eta=\"auto\",\n",
    "            iterations=iterations,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            eval_every=eval_every,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(\n",
    "            model=model,\n",
    "            texts=texts,\n",
    "            dictionary=dictionary,\n",
    "            coherence=\"c_v\",\n",
    "        )\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zWX2lGLJXxfu"
   },
   "outputs": [],
   "source": [
    "chunksize = 5000\n",
    "passes = 10\n",
    "iterations = 150\n",
    "eval_every = None\n",
    "start = THEME_COUNT_START\n",
    "limit = THEME_COUNT_END\n",
    "step = 1\n",
    "\n",
    "\n",
    "texts = [text.split() for text in X_train_src]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(\n",
    "    limit=limit,\n",
    "    passes=passes,\n",
    "    start=start,\n",
    "    step=step,\n",
    "    chunksize=chunksize,\n",
    "    iterations=iterations,\n",
    "    dictionary=dictionary,\n",
    "    corpus=corpus,\n",
    "    texts=texts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGHVlHfpXzJx"
   },
   "outputs": [],
   "source": [
    "x = range(start, limit + 1, step)\n",
    "\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy7PzmitYPzG"
   },
   "outputs": [],
   "source": [
    "best_model_index = coherence_values.index(max(coherence_values))\n",
    "best_model = model_list[best_model_index]\n",
    "\n",
    "lda_display = gensimvis.prepare(best_model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMLSkY-7YfVz"
   },
   "outputs": [],
   "source": [
    "def theme_vectorizer(\n",
    "    model: LdaModel,\n",
    "    dictionary: Dictionary,\n",
    "    X_train: pd.Series,\n",
    "    X_test: pd.Series,\n",
    "):\n",
    "    def topics_to_vector(topic_probs, num_topics):\n",
    "        vector = np.zeros(num_topics)\n",
    "        for topic_num, prob in topic_probs:\n",
    "            vector[topic_num] = prob\n",
    "        return vector\n",
    "\n",
    "    X_train_topics = [model[dictionary.doc2bow(text.split())] for text in X_train]\n",
    "    X_test_topics = [model[dictionary.doc2bow(text.split())] for text in X_test]\n",
    "\n",
    "    num_topics = model.num_topics\n",
    "\n",
    "    X_train_vectors = np.array(\n",
    "        [topics_to_vector(topics, num_topics) for topics in X_train_topics]\n",
    "    )\n",
    "    X_test_vectors = np.array(\n",
    "        [topics_to_vector(topics, num_topics) for topics in X_test_topics]\n",
    "    )\n",
    "\n",
    "    return X_train_vectors, X_test_vectors\n",
    "\n",
    "\n",
    "X_train_vectors, X_test_vectors = theme_vectorizer(\n",
    "    model=best_model,\n",
    "    dictionary=dictionary,\n",
    "    X_train=X_train_src,\n",
    "    X_test=X_test_src,\n",
    ")\n",
    "\n",
    "svm = SVC(probability=True, random_state=SEED)\n",
    "svm.fit(X_train_vectors, y_train_src)\n",
    "\n",
    "y_train_pred = svm.predict(X_train_vectors)\n",
    "y_test_pred = svm.predict(X_test_vectors)\n",
    "\n",
    "cl_rep_train = classification_report(y_train_src, y_train_pred, output_dict=True)\n",
    "cl_rep_test = classification_report(y_test_src, y_test_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT[\"svc\"] = {\n",
    "    \"accuracy_train\": cl_rep_train[\"accuracy\"],\n",
    "    \"accuracy_test\": cl_rep_test[\"accuracy\"],\n",
    "    \"model\": svm,\n",
    "    \"params\": None,\n",
    "    \"X_train\": X_train_vectors,\n",
    "    \"X_test\": X_test_vectors,\n",
    "    \"y_train\": y_train_src,\n",
    "    \"y_test\": y_test_src,\n",
    "    \"vectorizer\": None,\n",
    "    \"cl_rep_train\": cl_rep_train,\n",
    "    \"cl_rep_test\": cl_rep_test,\n",
    "}\n",
    "\n",
    "print(cl_rep_train[\"accuracy\"], cl_rep_test[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIqdoQPlZE04"
   },
   "source": [
    "## 4. Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_src)\n",
    "y_test_encoded = label_encoder.transform(y_test_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "v5nnolstdKID"
   },
   "outputs": [],
   "source": [
    "if platform.system() == \"Windows\":\n",
    "    task_type = \"GPU\"\n",
    "    \n",
    "else:\n",
    "    task_type = \"CPU\"\n",
    "\n",
    "def objective(trial, X, y, cv_folds=5):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", MIN_ITERS, MAX_ITERS),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 7, 15),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 100.0, log=True),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 30),\n",
    "        \"loss_function\": \"MultiClass\",\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=cv_folds,\n",
    "        shuffle=True,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_valid = X[train_idx], X[valid_idx]\n",
    "        y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            verbose=0,\n",
    "            task_type=task_type,\n",
    "            random_state=SEED,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        fold_score = f1_score(y_valid, preds, average=\"weighted\")\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def train_catboost_with_optuna(\n",
    "    name: str,\n",
    "    vectorizer,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_trials: int = 100,\n",
    "    cv_folds: int = 5,\n",
    "    params: dict = None,\n",
    "    timeout: float = None,\n",
    "    force: bool = False,\n",
    "):  \n",
    "    def go_study():\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "        study.optimize(\n",
    "            lambda trial: objective(\n",
    "                trial=trial,\n",
    "                X=X_train,\n",
    "                y=y_train,\n",
    "                cv_folds=cv_folds,\n",
    "            ),\n",
    "            n_trials=n_trials,\n",
    "            timeout=timeout,\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "\n",
    "        return study.best_params\n",
    "\n",
    "    if force == False:\n",
    "        if params == None:\n",
    "            if name in RESULT:\n",
    "                print(\n",
    "                    f\"name: {name}, train: {RESULT[name]['accuracy_train']}, test: {RESULT[name]['accuracy_test']}\"\n",
    "                )\n",
    "                return RESULT[name][\"model\"]\n",
    "            else:\n",
    "                params = go_study()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        if not params:\n",
    "            params = go_study()\n",
    "\n",
    "    final_model = CatBoostClassifier(\n",
    "        **params,\n",
    "        verbose=0,\n",
    "        task_type=task_type,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = final_model.predict(X_train)\n",
    "    y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "    cl_rep_train = classification_report(y_train, y_train_pred, output_dict=True)\n",
    "    cl_rep_test = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "\n",
    "    RESULT[name] = {\n",
    "        \"accuracy_train\": cl_rep_train[\"accuracy\"],\n",
    "        \"accuracy_test\": cl_rep_test[\"accuracy\"],\n",
    "        \"model\": final_model,\n",
    "        \"params\": params,\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"cl_rep_train\": cl_rep_train,\n",
    "        \"cl_rep_test\": cl_rep_test,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"name: {name}, train: {cl_rep_train[\"accuracy\"]}, test: {cl_rep_test[\"accuracy\"]}\"\n",
    "    )\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFUDBWyyctcO"
   },
   "source": [
    "1. В методах преобразования данных пробуйте различные параметры, в поисках лучших для решения текущей задачи\n",
    "2. Не забывайте про подбор параметров у самого бустинга с помощью optuna\n",
    "\n",
    "https://forecastegy.com/posts/catboost-hyperparameter-tuning-guide-with-optuna/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQq_IsefbVRG"
   },
   "source": [
    "### Мешок слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCGj9L8WY2uy"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_df=MAX_DF,\n",
    "    min_df=MIN_DF,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_src)\n",
    "X_test = vectorizer.transform(X_test_src)\n",
    "\n",
    "print(X_train.shape, MIN_DF, MAX_DF)\n",
    "\n",
    "train_catboost_with_optuna(\n",
    "    name=\"bow\",\n",
    "    vectorizer=vectorizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_encoded,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test_encoded,\n",
    "    n_trials=N_TRIALS,\n",
    "    cv_folds=CV_FOLDS,\n",
    "    # params=RESULT[\"bow\"][\"params\"] if \"bow\" in RESULT else None,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mK0p4OnbcWQ"
   },
   "source": [
    "### Мешок слов + n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxupD4dQbep1"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_df=MAX_DF,\n",
    "    min_df=MIN_DF,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_src)\n",
    "X_test = vectorizer.transform(X_test_src)\n",
    "\n",
    "train_catboost_with_optuna(\n",
    "    name=\"bow_n\",\n",
    "    vectorizer=vectorizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_encoded,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test_encoded,\n",
    "    n_trials=N_TRIALS,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUHWGB0Qbcv0"
   },
   "source": [
    "### Мешок слов + m-skip-n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDi7CTwDbkWD"
   },
   "outputs": [],
   "source": [
    "class SkipGramVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "            compose(\n",
    "                tokenize,\n",
    "                preprocess,\n",
    "                self.decode,\n",
    "            )(doc)\n",
    "        )\n",
    "\n",
    "    def _word_skip_grams(self, tokens):\n",
    "        return compose(cmap(\" \".join), pluck([0, 2]), sliding_window(3))(tokens)\n",
    "\n",
    "\n",
    "vectorizer = SkipGramVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_df=MAX_DF,\n",
    "    min_df=MIN_DF,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_src)\n",
    "X_test = vectorizer.transform(X_test_src)\n",
    "\n",
    "train_catboost_with_optuna(\n",
    "    name=\"bow_n_m\",\n",
    "    vectorizer=vectorizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_encoded,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test_encoded,\n",
    "    n_trials=N_TRIALS,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf2RrhA9bc_h"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUT_wQbzbwlM"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_df=MAX_DF,\n",
    "    min_df=MIN_DF,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_src)\n",
    "X_test = vectorizer.transform(X_test_src)\n",
    "\n",
    "train_catboost_with_optuna(\n",
    "    name=\"tfidf\",\n",
    "    vectorizer=vectorizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_encoded,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test_encoded,\n",
    "    n_trials=N_TRIALS,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nkX0ir9bxR-"
   },
   "source": [
    "### TF-IDF + n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNjczhqrbxfW"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_df=MAX_DF,\n",
    "    min_df=MIN_DF,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_src)\n",
    "X_test = vectorizer.transform(X_test_src)\n",
    "\n",
    "train_catboost_with_optuna(\n",
    "    name=\"tfidf_n\",\n",
    "    vectorizer=vectorizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_encoded,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test_encoded,\n",
    "    n_trials=N_TRIALS,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DsvDmkDbx3v"
   },
   "source": [
    "### TF-IDF + m-skip-n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXrUHVlhbyEg"
   },
   "outputs": [],
   "source": [
    "class SkipGramVectorizerTf(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        preprocess = self.build_preprocessor()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "            compose(\n",
    "                tokenize,\n",
    "                preprocess,\n",
    "                self.decode,\n",
    "            )(doc)\n",
    "        )\n",
    "\n",
    "    def _word_skip_grams(self, tokens):\n",
    "        return compose(cmap(\" \".join), pluck([0, 2]), sliding_window(3))(tokens)\n",
    "\n",
    "\n",
    "vectorizer = SkipGramVectorizerTf(\n",
    "    max_features=MAX_FEATURES,\n",
    "    max_df=MAX_DF,\n",
    "    min_df=MIN_DF,\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_src)\n",
    "X_test = vectorizer.transform(X_test_src)\n",
    "\n",
    "train_catboost_with_optuna(\n",
    "    name=\"tfidf_n_m\",\n",
    "    vectorizer=vectorizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_encoded,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test_encoded,\n",
    "    n_trials=N_TRIALS,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sa_FLN--UKhx"
   },
   "source": [
    "### Генерация искусственных данных и балансировка классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itbqzh7gUS2q"
   },
   "source": [
    "Выберите лучшее представление данных, опираясь на метрику. Попробуйте сбалансировать классы с помощью весов (параметр catboost), если выше этого не делали. Попробуйте сгенерировать новые данные для классов, в которых меньше всего объектов. Генерация представляет собой семплирование токенов из всего множества токенов определенного класса. Обучите модель на новом датасете, сравните качество.\n",
    "\n",
    "1. признаки предложений (длина предложений, наличие орф.знаков, количество предложений в тексте)\n",
    "2. обработка опечаток (расстояние между словом нормальным и словом с опечаткой)\n",
    "3. склеивание с частицей не."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_catboost_model_name = None\n",
    "mx_score = -np.inf\n",
    "\n",
    "for key in RESULT.keys():\n",
    "    if key == \"svc\":\n",
    "        continue\n",
    "\n",
    "    if RESULT[key][\"accuracy_test\"] > mx_score:\n",
    "        mx_score = RESULT[key][\"accuracy_test\"]\n",
    "        best_catboost_model_name = key\n",
    "\n",
    "best_catboost_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# балансировка\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    \"balanced\",\n",
    "    classes=np.unique(y_train_encoded),\n",
    "    y=y_train_encoded,\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "params = RESULT[best_catboost_model_name][\"params\"] | {\n",
    "    \"class_weights\": class_weights_dict,\n",
    "}\n",
    "\n",
    "model = CatBoostClassifier(**params)\n",
    "\n",
    "model.fit(RESULT[best_catboost_model_name][\"X_train\"], y_train_encoded, silent=True)\n",
    "\n",
    "y_train_pred = model.predict(RESULT[best_catboost_model_name][\"X_train\"])\n",
    "y_test_pred = model.predict(RESULT[best_catboost_model_name][\"X_test\"])\n",
    "\n",
    "cl_rep_train = classification_report(y_train_encoded, y_train_pred, output_dict=True)\n",
    "cl_rep_test = classification_report(y_test_encoded, y_test_pred, output_dict=True)\n",
    "\n",
    "RESULT[\"balanced\"] = {\n",
    "    \"accuracy_train\": cl_rep_train[\"accuracy\"],\n",
    "    \"accuracy_test\": cl_rep_test[\"accuracy\"],\n",
    "    \"model\": model,\n",
    "    \"params\": params,\n",
    "    \"X_train\": RESULT[best_catboost_model_name][\"X_train\"],\n",
    "    \"X_test\": RESULT[best_catboost_model_name][\"X_test\"],\n",
    "    \"y_train\": y_train_encoded,\n",
    "    \"y_test\": y_test_encoded,\n",
    "    \"vectorizer\": vectorizer,\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"name: balanced, train: {cl_rep_train[\"accuracy\"]}, test: {cl_rep_test[\"accuracy\"]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация искусственных данных\n",
    "\n",
    "class_counts = df[\"category\"].value_counts()\n",
    "mean_count = class_counts.mean()\n",
    "underrepresented = class_counts[class_counts < mean_count]\n",
    "\n",
    "mean_token_len = defaultdict(list)\n",
    "class_tokens = defaultdict(list)\n",
    "\n",
    "for category, text in zip(df[\"category\"], df[\"processed_text\"]):\n",
    "    tokens = text.split()\n",
    "    mean_token_len[category].append(len(tokens))\n",
    "    class_tokens[category].extend(tokens)\n",
    "\n",
    "synthetic_texts = []\n",
    "synthetic_labels = []\n",
    "\n",
    "for category in underrepresented.index:\n",
    "    samples_to_generate = int(mean_count - class_counts[category])\n",
    "    tokens_pool = class_tokens[category]\n",
    "\n",
    "    avg_len = int(sum(mean_token_len[category]) / len(mean_token_len[category]))\n",
    "\n",
    "    print(category, avg_len)\n",
    "\n",
    "    for _ in range(samples_to_generate):\n",
    "        num_tokens = random.randint(avg_len - 2, avg_len + 2)\n",
    "        synthetic_text = \" \".join(random.choices(tokens_pool, k=num_tokens))\n",
    "        synthetic_texts.append(process_text(synthetic_text))\n",
    "        synthetic_labels.append(category)\n",
    "\n",
    "print()\n",
    "print(len(synthetic_texts), len(synthetic_labels), \"\\n\")\n",
    "print(synthetic_texts[:5], synthetic_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(text):\n",
    "    sentences = text.split(\".\")\n",
    "    return {\n",
    "        \"token_count\": len(text.split()),\n",
    "        \"avg_token_len\": np.mean([len(token) for token in text.split()]),\n",
    "        \"sentence_length\": np.mean([len(s.split()) for s in sentences if s.strip()]),\n",
    "        # \"punctuation_count\": sum(1 for c in text if c in string.punctuation), не нужно, так как знаки чистятся\n",
    "    }\n",
    "\n",
    "\n",
    "df_augmented = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": list(df[\"processed_text\"]) + synthetic_texts,\n",
    "        \"category\": list(df[\"category\"]) + synthetic_labels,\n",
    "    }\n",
    ")\n",
    "\n",
    "text_features = pd.DataFrame([get_text_features(text) for text in df_augmented[\"text\"]])\n",
    "\n",
    "df_augmented = pd.concat([df_augmented, text_features], axis=1)\n",
    "\n",
    "df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = RESULT[best_catboost_model_name][\"vectorizer\"]\n",
    "\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
    "    df_augmented.drop(\"category\", axis=1),\n",
    "    df_augmented[\"category\"],\n",
    "    random_state=SEED,\n",
    "    stratify=df_augmented[\"category\"],\n",
    ")\n",
    "\n",
    "y_train_aug_encoded = label_encoder.fit_transform(y_train_aug)\n",
    "y_test_aug_encoded = label_encoder.transform(y_test_aug)\n",
    "\n",
    "X_train = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(vectorizer.fit_transform(X_train_aug[\"text\"]).A).reset_index(\n",
    "            drop=True\n",
    "        ),\n",
    "        pd.DataFrame(X_train_aug.drop(\"text\", axis=1).values.tolist()),\n",
    "    ],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ").values\n",
    "\n",
    "X_test = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(vectorizer.transform(X_test_aug[\"text\"]).A).reset_index(drop=True),\n",
    "        pd.DataFrame(X_test_aug.drop(\"text\", axis=1).values.tolist()),\n",
    "    ],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ").values\n",
    "\n",
    "train_catboost_with_optuna(\n",
    "    name=\"augmented\",\n",
    "    vectorizer=vectorizer,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_aug_encoded,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test_aug_encoded,\n",
    "    n_trials=N_TRIALS,\n",
    "    cv_folds=CV_FOLDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hv_sCKBUdF-"
   },
   "source": [
    "### Понижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcEsTtMZUy-m"
   },
   "source": [
    "- уменьшите размерность векторов с помощью PCA, посмотрите, улучшается ли качество\n",
    "- попробуйте несколько вариантов понижения размерности: от 90% до 50%\n",
    "от изначального размера вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "wYIuExQvspoq"
   },
   "outputs": [],
   "source": [
    "def evaluate_with_pca(\n",
    "    X_train_vec,\n",
    "    X_test_vec,\n",
    "    dim_reduction_ratios=[0.9, 0.7, 0.5],\n",
    "):\n",
    "    results = {}\n",
    "    n_features = X_train_vec.shape[1]\n",
    "\n",
    "    for ratio in dim_reduction_ratios:\n",
    "        n_components = int(n_features * ratio)\n",
    "        vectorizer = PCA(n_components=n_components)\n",
    "\n",
    "        X_train = vectorizer.fit_transform(X_train_vec)\n",
    "        X_test = vectorizer.transform(X_test_vec)\n",
    "\n",
    "        train_catboost_with_optuna(\n",
    "            name=f\"{best_catboost_model_name}_pca_{ratio}\",\n",
    "            vectorizer=vectorizer,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train_encoded,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test_encoded,\n",
    "            n_trials=N_TRIALS,\n",
    "            cv_folds=CV_FOLDS,\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_with_pca(\n",
    "    RESULT[best_catboost_model_name][\"X_train\"],\n",
    "    RESULT[best_catboost_model_name][\"X_test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L07K_QeldmMf"
   },
   "source": [
    "## 5. Интерпретация результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnqAUg_bdxJZ"
   },
   "source": [
    "https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Catboost%20tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "rzVYFgV1dqnL"
   },
   "outputs": [],
   "source": [
    "# используя туториал выше, интерпретируйте результаты.\n",
    "# Определите, какие слова влияют в каждом классе больше остальных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(RESULT[best_catboost_model_name][\"model\"], seed=SEED)\n",
    "sv = explainer(RESULT[best_catboost_model_name][\"X_test\"])\n",
    "\n",
    "feature_names = (\n",
    "    vectorizer.get_feature_names_out()\n",
    "    if hasattr(vectorizer, \"get_feature_names_out\")\n",
    "    else vectorizer.get_feature_names()\n",
    ")\n",
    "\n",
    "sv.feature_names = feature_names\n",
    "\n",
    "for cat_id in np.unique(y_test_encoded):\n",
    "    plt.figure()\n",
    "    sp.beeswarm(sv[:, :, cat_id], max_display=10, show=False)\n",
    "    plt.title(f\"Category: {label_encoder.classes_[cat_id]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0JBHJDGUs8t"
   },
   "source": [
    "### Визуализация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzUmS-IkUvgc"
   },
   "outputs": [],
   "source": [
    "# С помощью методов понижения размерности T-SNE И U-MAP взгляните\n",
    "# на получившиеся векторные представления данных\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\")\n",
    "X_tsne = tsne.fit_transform(RESULT[best_catboost_model_name][\"X_test\"])\n",
    "\n",
    "umap = UMAP(n_components=2)\n",
    "X_umap = umap.fit_transform(RESULT[best_catboost_model_name][\"X_test\"])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(\n",
    "    X_tsne[:, 0],\n",
    "    X_tsne[:, 1],\n",
    "    c=y_test_encoded,\n",
    ")\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(\n",
    "    X_umap[:, 0],\n",
    "    X_umap[:, 1],\n",
    "    c=y_test_encoded,\n",
    ")\n",
    "plt.title(\"UMAP Visualization\")\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dt = datetime.now()\n",
    "\n",
    "end_dt - start_dt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJbai/zinIlfO6Z++x5N8X",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
